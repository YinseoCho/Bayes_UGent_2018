Introduction to Bayesian inference
========================================================
author: Ladislas Nalborczyk
date: Univ. Grenoble Alpes, CNRS, LPNC (France) • Ghent University (Belgium)
autosize: true
transition: none
width: 1600
height: 1000
css: css-file.css

Probabilistic interpretations
========================================================
incremental: true
type: lineheight

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
```

<!-- For syntax highlighting -->
<link rel="stylesheet" href="github.css">

What is the probability...

* Of getting head when flipping a coin ?
* That I learn something new during this course ?

Do these two questions refer to the same concept of *probability* ?

```{r echo = FALSE, fig.align = "center", out.width = "600px"}
knitr::include_graphics("thinking.gif")
```

Probability axioms
========================================================
incremental: true
type: lineheight

A probability is a numerical value assigned to an event $A$, this event being a possibility of an ensemble $\Omega$ (the ensemble of all possible events). Probabilities conform to the following axioms (the third axiom is only valid for mutually exclusive events):

+ **Non-negativity.** $P(A_{i}) \geq 0$
+ **Normalisation.** $P(\Omega) = 1$
+ **Additivity.** $P(A_{1}\cup A_{2}\cup \ldots) = \sum_{i=1}^{+\infty}P(A_{i})$

<small>*Kolmogorov, A.N. (1933). "Foundations of the Theory of Probability”.*</small>

Classical (or theoretical) interpretation
========================================================
incremental: true
type: lineheight

$$P(even)=\frac{number\ of\ favorable\ cases}{number\ of\ possible\ cases}=\frac{3}{6}=\frac{1}{2}$$

Problem: this definition only applies to situations in which there is a finite ensemble of equiprobable events...

> What is the probability of raining tomorrow ?

> $$P(rain)=\frac{rain}{ \{ rain,\ non-rain \} }=\frac{1}{2}??$$

Frequentist (or empirical) interpretation
========================================================
incremental: true
type: lineheight

$$P(x)=\lim_{n_{t} \to \infty}\frac{n_{x}}{n_{t}}$$

Where $n_{x}$ is the number of events $x$ and $n_{t}$ the total number of trials. The **frequentist** interpretation postulates that, **in the long run** (when the number of trial approaches infinity), the relative frequency will eventually converge to what we call *probability*.

Consequence: the concept or *probability* only applies to collectives, and not to singular events.

Frequentist (or empirical) interpretation
========================================================
incremental: false
type: lineheight
    
```{r, message = FALSE, fig.width = 8, fig.heigth = 4, fig.align = "center"} 
library(tidyverse)

sample(c(0, 1), 500, replace = TRUE) %>%
        data.frame %>%
        mutate(x = seq_along(.), y = cumsum(.) / seq_along(.) ) %>%
        ggplot(aes(x = x, y = y), log = "y") +
        geom_line(lwd = 1) +
        geom_hline(yintercept = 0.5, lty = 3) +
        xlab("number of trials") +
        ylab("proportion of heads") +
        ylim(0, 1) +
        theme_bw(base_size = 20)
```

Problems...
========================================================
incremental: true
type: lineheight

- What is the reference class ? *What is the probability of me living until 80 years ?*

- What about non-repeatable events ? *What is the probability of learning something new during this course ?*

- How many trials are sufficient to obtain a good approximation of the *probability* ? A finite class of events of size $n$ can only produce relative frequencies of certain preciseness $1/n$.

Propensity interpretation
========================================================
incremental: true
type: lineheight

The frequentist properties (in the long run) of objects (e.g., a coin) might be produced by intrinsic properties of these objects. For instance, a biased coin will produce a biased relative frequency **because** of its intrinsic bias.

The propensity interpretation postulates that these intrinsic properties define what we call as probability. In other words, the probability of an event = its intrinsic properties.

This interpretation will usually agree with the frequentist interpretation but it shift the locus of probability from the external world (the observable relative frequency in the long run) to internal properties of objects.

Consequence: these properties are properties that are intrinsic to objects... The propensity interpretation then allows to talk about the probability of single events.

Logical interpretation
========================================================
incremental: true
type: lineheight

<center>There are 10 students in this room</center>
<center>9 students wear a <font color = "green">green</font> t-shirt</center>
<center>1 student wears a <font color = "red">red</font> t-shirt</center>
<center>Let's say 1 person is picked at random...</center>

<hr width = "75%%" size = "2" align = "center" noshade>

> <center>Conclusion 1: this student wears a t-shirt &#10004; </center>

<hr width = "75%%" size = "1" align = "center" noshade>

> <center>Conclusion 2: this student wears a <font color = "green">green t-shirt</font> &#10007; </center>

<hr width = "75%%" size = "1" align = "center" noshade>

> <center>Conclusion 3: this student wears a <font color = "red">red t-shirt</font> &#10007; </center>

Logical interpretation
========================================================
incremental: false
type: lineheight

The logical interpretation of probability aims at generalising the formal logic (true / false) to the probabilist world. The probability then represents the degree of *logical support* of a conclusion, in relation to a set of premises ([Keynes, 1921](https://archive.org/details/treatiseonprobab007528mbp); [Carnap, 1950](http://fitelson.org/confirmation/carnap_logical_foundations_of_probability.pdf)).

Consequence: every probability is **conditional**.

Bayesian interpretation
========================================================
incremental: true
type: lineheight

The probability is a way of **quantifying uncertainty**. A certain event then has a probability of 1 and an impossible event has a probability of 0.

This interpretation eliminates the need for a collective of infinite repretition of an event...

> *So to assign equal probabilities to two events is not in any way an assertion that they must occur equally often in any random experiment [...], it is only a formal way of saying I don’t know* ([Jaynes, 1986](http://bayes.wustl.edu/etj/articles/general.background.pdf)).

Probabilistic interpretations
========================================================
incremental: false
type: lineheight

+ Classic interpretation (Laplace, Bernouilli, Leibniz)
+ **Frequentist interpretation** (Venn, Reichenbach, von Mises)
+ Propensity interpretation (Popper, Miller)
+ Logical interpretation (Keynes, Carnap)
+ **Bayesian interpretation** (Jeffreys, de Finetti, Savage)

*[Click for more details...](http://plato.stanford.edu/entries/probability-interpret/)*

Probabilistic interpretations - summary
========================================================
incremental: true
type: lineheight

> **Epistemic probability**
- every probability is conditional on some available information (e.g., premises or data)
- probabilities as a way of quantifying uncertainty
- logical interpretation, Bayesian interpretation

<hr style="height:20pt; visibility:hidden;" />

> **Physical probability**
- probabilities depend on a state of the world, on physical characteristics. They are independent of knowledge or uncertainty.
- classic interpretation, frequentist interpretation</center>

========================================================
type: black

<img src="pill.jpg" height="1000px" width="1600px" />

What about randomness ?
========================================================
type: lineheight
incremental: true

Let's pick a series of numbers *at random*...

```{r eval = TRUE, echo = FALSE, fig.align = "center"}
set.seed(666)
as.integer(runif(5, 10, 100) )
```

`runif, rnorm, rbinom...`

```{r eval = TRUE, echo = TRUE, fig.align = "center"}
RNGkind()[1] # default pseudorandom number generator in R
```

Mersenne-Twister
========================================================
type: lineheight
incremental: false

Introduced by Makoto Matsumoto and Takuji Nishimura in 1997, the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister) algorithm is a pseudo-random numbers generator (PRNG). It is the default algorithm in Python, Ruby, R, PHP, Matlab...

<div align = "center">
<img src = "mersenne.png" width = 1200 height = 600>
</div>

Determinism and randomness
========================================================
type: lineheight
incremental: true

$$ Y_{i} \sim \mathrm{Uniform}(10,100) $$

```{r eval = TRUE, echo = TRUE, fig.align = "center"}
set.seed(666)
as.integer(runif(5, 10, 100) )
```

If I know the *seed* (i.e., where the algorithm starts) and the detailed mechanisms of the algorithm, I can predict without errors the numbers that wil be generated. Then, it would be difficult to maintain that these numbers have been generated *at random*...

```{r eval = TRUE, echo = FALSE, fig.align = "center"}
set.seed(666)
as.integer(runif(6, 10, 100) )
```

```{r eval = TRUE, echo = FALSE, fig.align = "center"}
set.seed(666)
as.integer(runif(7, 10, 100) )
```

A language for uncertainty
========================================================
type: lineheight
incremental: true

*Randomness is the lack of pattern or predictability in events. A random sequence of events, symbols or steps has no order and does not follow an intelligible pattern or combination* ([Wikipédia](https://en.wikipedia.org/wiki/Randomness)).

> "Ce que nous appelons hasard n’est et ne peut être que la cause ignorée d’un effet connu." -- Voltaire

> "Le hasard, ce sont les lois que nous ne connaissons pas." -- Émile Borel

> "Randomness is a proxy for lack of knowledge" -- Richard McElreath

Randomness can be defined as a *subjective state of causal indetermination*. To talk about uncertain events (to quantify uncertainty), we use probabilities.

A piece of formal logic
========================================================
type: lineheight
incremental: true

<div align = "center" style="float: bottom;">
<img src = "penguin.jpg" width = 600 height = 600>
</div>

A piece of formal logic
========================================================
type: lineheight
incremental: true

**Example n°1**

- If a suspect is lying, he will sweat. He is sweating.
- So, he is lying.

**Example n°2**

- If a suspect is sweating, he is lying. He is not sweating.
- So, he is not lying.

**Example n°3**

- All liars are sweating. This suspect is not sweating.
- So, he is not a liar.

Invalid inferences
========================================================
type: lineheight
incremental: true

- *affirming the consequent*: $\dfrac{A \Rightarrow B, \ B}{A}$

- If it rains, the ground is wet (A implies B). The ground is wet (B). Then, it rained (A).

- *denying the antecedent*: $\dfrac{A \Rightarrow B, \ \neg A}{\neg B}$

- If it rains, the ground is wet (A implies B). It did not rain (non A). Then, the ground is not wet (non A).

```{r echo = FALSE, fig.align = "center", out.width = "600px"}
knitr::include_graphics("trump.gif")
```

Valid inferences
========================================================
type: lineheight
incremental: true

- *modus ponens*: $\dfrac{A \Rightarrow B, \ A}{B}$

- If today is Monday, then John will go to work (A implies B). Today is Monday (A). Then John will go to work (B).

- *modus tollens*: $\dfrac{A \Rightarrow B, \ \neg B}{\neg A}$

- If my dog feels the presence of an intruder, he will bark (A implies B). My dog did not bark (not B). Then, he did not feel the presence of an intruder (not A).

```{r echo = FALSE, fig.align = "center", out.width = "400px"}
knitr::include_graphics("good.gif")
```

Logic, frequentism and probabilistic reasoning
========================================================
incremental: true
type: lineheight

The *modus tollens* is one of the strongest rule of inference in logic. It works perfectly well in science when we deal with hypotheses of the following form: *If $H_{0}$ is true, then we should not observe $x$. We observed $x$. Then, $H_{0}$ is false*.

BUT, most of the time, we deal with *continuous*, *probabilistic* hypotheses...

The Fisherian inference (induction) is of the form: *If $H_{0}$ is true, then we should PROBABLY not observe $x$. We observed $x$. Then, $H_{0}$ is PROBABLY false*.

But this argument is invalid. The *modus tollens* does not apply to probabilistic statements (e.g., [Pollard & Richardson, 1987](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.505.9968&rep=rep1&type=pdf); [Rouder, Morey, Verhagen, Province, & Wagenmakers, 2016](http://www.ejwagenmakers.com/2016/RouderEtAl2016FreeLunch.pdf)).

Let's take an example: *If Jane is an American, then it is unlikely that she is a U.S. Congressperson. Jane is a U.S. Congressperson. Then, Jane is probably not an American...*

The failure of strict falsificationism
========================================================
type: lineheight
incremental: true

*Poppérisme naïf*: la science progresse par falsification logique, donc la statistique devrait viser la falsification. Mais...

- les hypothèses (théoriques) ne sont pas les modèles (hypothèses statistiques)

> "Models are devices that connect theories to data. A model is an instanciation of a theory as a set of probabilistic statements" -- (Rouder et al., 2016)

- nos hypothèses sont souvent probabilistes
- erreur de mesure
- la falsification concerne le problème de la démarcation, pas celui de la méthode
- la science est une technologie sociale, la falsification est **consensuelle**, et non pas logique

Notre stratégie
========================================================
type: lineheight
incremental: true

Besoin d'un cadre pour développer des modèles cohérents. Nos outils:

- *Bayesian data analysis*, utiliser les probabilités pour décrire l'incertitude, et étendre la logique au monde probabiliste
- *multilevel modeling*, des modèles à multiples niveaux d'incertitude
- approche par comparaison de modèle, au lieu d'essayer de falsifier un *null model*, comparer des modèles intéressants (AIC, WAIC)

Comparaison de modèles
========================================================
type: lineheight
incremental: false

On s'intéresse au lien entre deux variables aléatoires continues, $x$ et $y$.

```{r message = FALSE, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
set.seed(1111)

x <- sort(runif(10, -2, 2) )
y <- 3 * x^3 + 5 * x^2 + 0.5 * x + 20 + rnorm(10, sd = 3) # a 3 polynomial model

data.frame(x, y) %>%
        ggplot(aes(x = x, y = y) ) +
        geom_point(size = 3) +
        theme_bw(base_size = 20)

nterm <- c(1, 2, 3, 9)

PAL <- colorRampPalette(c("black", "chartreuse3", "gold", "dodgerblue") )
COLS <- PAL(length(nterm) )
```

Comparaison de modèles
========================================================
type: lineheight
incremental: false

L'hypothèse de modélisation la plus simple est de postuler une relation linéaire.

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
data.frame(x, y) %>%
        ggplot(aes(x = x, y = y) ) +
        geom_point(size = 3) +
        geom_smooth(method = "lm", se = FALSE, col = COLS[1]) +
        theme_bw(base_size = 20)
```

Comparaison de modèles
========================================================
type: lineheight
incremental: false

Cette description peut-être *améliorée* pour mieux prendre en compte les données qui s'écartent de la prédiction linéaire.

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
data.frame(x, y) %>%
        ggplot(aes(x = x, y = y) ) +
        geom_point(size = 3) +
        geom_smooth(method = "lm", se = FALSE, col = COLS[1]) +
        stat_smooth(
                method = "lm", se = FALSE,
                formula = y ~ poly(x, 2),
                col = COLS[2]) +
        theme_bw(base_size = 20)
```

Comparaison de modèles
========================================================
type: lineheight
incremental: true

Un ensemble de $N$ points peut être *exhaustivement* (sans erreur) décrit par une fonction polynomiale d'ordre $N-1$. Augmenter la complexité du modèle améliore donc la précision de notre description des données mais réduit la généralisabilité de ses prédictions (*bias-variance tradeoff*).

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
data.frame(x, y) %>%
        ggplot(aes(x = x, y = y) ) +
        geom_point(size = 3) +
        geom_smooth(method = "lm", se = FALSE, col = COLS[1]) +
        stat_smooth(
                method = "lm", se = FALSE,
                formula = y ~ poly(x, 2),
                col = COLS[2]) +
        stat_smooth(
                method = "lm", se = FALSE,
                formula = y ~ poly(x, 3),
                col = COLS[3]) +
        stat_smooth(
                method = "lm", se = FALSE,
                formula = y ~ poly(x, 9),
                col = COLS[4]) +
        theme_bw(base_size = 20)
```

Nous avons besoin d'outils qui prennent en compte le rapport *qualité de la description* / *complexité*, c'est à dire la **parcimonie** des modèles (AIC, WAIC).
 
Inférence bayésienne
========================================================
type: lineheight
incremental: true

Dans ce cadre, pour chaque problème, nous allons suivre 3 étapes:

- Construire le modèle (l'histoire des données): *likelihood* + *prior*
- Mettre à jour grâce aux données (*updating*), calculer la probabilité *a posteriori*
- Evaluer le modèle, *fit*, sensibilité, résumer les résultats, ré-ajuster

> "Bayesian inference is really just counting and comparing of possibilities [...] in order to make good inference about what actually happened, it helps to consider everything that could have happened." -- McElreath (2015).

Exercice - Problème du sac de billes
========================================================
type: lineheight
incremental: true

Imaginons que nous disposions d'un sac, contenant 4 billes. Ces billes peuvent être soit blanches, soit bleues. Nous savons qu'il y a précisémment 4 billes, mais nous ne connaissons pas le nombre de billes blanches, ou bleues.

Nous savons cependant qu'il existe cinq possibilités (que nous considérerons comme nos *hypothèses*):

> <p align = "center"> &#9898 &#9898 &#9898 &#9898</p>
> <p align = "center"> &#128309 &#9898 &#9898 &#9898</p>
> <p align = "center">&#128309 &#128309 &#9898 &#9898</p>
> <p align = "center">&#128309 &#128309 &#128309 &#9898</p>
> <p align = "center">&#128309 &#128309 &#128309 &#128309</p>

Exercice - Problème du sac de billes
========================================================
type: lineheight
incremental: true

Le but est de déterminer quelle combinaison serait la plus probable, **sachant certaines observations**. Imaginons que l'on tire trois billes à la suite, avec remise, et que l'on obtienne la séquence suivante:

<p align = "center">&#128309 &#9898 &#128309</p>

Cette séquence représente nos données. A partir de là, quelle **inférence** peut-on faire sur le contenu du sac ? En d'autres termes, que peut-on dire de la probabilité de chaque hypothèse ?

> <p align = "center"> &#9898 &#9898 &#9898 &#9898</p>
> <p align = "center"> &#128309 &#9898 &#9898 &#9898</p>
> <p align = "center">&#128309 &#128309 &#9898 &#9898</p>
> <p align = "center">&#128309 &#128309 &#128309 &#9898</p>
> <p align = "center">&#128309 &#128309 &#128309 &#128309</p>

Enumérer les possibilités
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothèse: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; données: &#128309 </p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
library(rethinking)
source("forking_data_McElreath.R")

dat <- c(1)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 1)
```

Enumérer les possibilités
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothèse: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; données: &#128309 &#9898</p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
library(rethinking)
source("forking_data_McElreath.R")

dat <- c(1, 0)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 1)
```

Enumérer les possibilités
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothèse: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; données: &#128309 &#9898 &#128309</p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
library(rethinking)
source("forking_data_McElreath.R")

dat <- c(1, 0, 1)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 1)
```

Enumérer les possibilités
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothèse: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; données: &#128309 &#9898 &#128309 </p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}

library(rethinking)
source("forking_data_McElreath.R")

dat <- c(1,0,1)
arc <- c(0,pi)

garden(
        arc = arc,
        possibilities = c(0,0,0,1),
        data = dat,
        hedge = 0.05,
        ring_dist = ring_dist,
        alpha.fade = 0.3
)

```

Enumérer les possibilités
========================================================
incremental: false
type: lineheight

Sous cette hypothèse, $3$ chemins (sur $4^{3}=64$) conduisent au résultat obtenu. Qu'en est-il des autres hypothèses?

<p align = "center"> &#9898 &#9898 &#9898 &#128309 &emsp;&emsp;&emsp;&emsp; &#9898 &#128309 &#128309 &#128309 &emsp;&emsp;&emsp;&emsp; &#9898 &#9898 &#128309 &#128309 </p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
source("forking_data_McElreath.R")

dat <- c(1, 0, 1)
ac <- c(1.2, 0.9, 0.6)

arc <- c( pi / 2, pi / 2 + (2 / 3) * pi)

garden(
    arc = arc,
    possibilities = c(1, 0, 0, 0),
    data = dat,
    hedge = 0.05,
    adj.cex = ac) 

arc <- c(arc[2], arc[2] + (2 / 3) * pi)

garden(
    arc = arc,
    possibilities = c(1, 1, 0, 0),
    data = dat,
    hedge = 0.05,
    newplot = FALSE,
    adj.cex = ac)

arc <- c(arc[2], arc[2] + (2 / 3) * pi)

garden(
    arc = arc,
    possibilities = c(1, 1, 1, 0),
    data = dat,
    hedge = 0.05,
    newplot = FALSE,
    adj.cex = ac)

line.polar(c(0, 2), pi / 2, lwd = 1)
line.polar(c(0, 2), pi / 2 + (2 / 3) * pi, lwd = 1)
line.polar(c(0, 2), pi / 2 + 2 * (2 / 3) * pi, lwd = 1)
```

Comparer les hypothèses
========================================================
incremental: false
type: lineheight

<p align = "center"> &#128309 &#128309 &#128309 &#9898 </p>

Au vu des données, cette hypothèse est la plus *probable* car c'est l'hypothèse qui maximise le nombre de manières possibles d'obtenir les données obtenues.

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:30px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:30px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothèse</th>
    <th class="tg-gq9a">Façons d'obtenir les données</th>
  </tr>
  <tr>
    <td class="tg-gq9a"> <p> &#9898 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a"> 0 x 4 x 0 = 0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">1 x 3 x 1 = 3</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">2 x 2 x 2 = 8 </td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#128309 &#9898 </p></td>
    <td class="tg-gq9a">3 x 1 x 3 = 9</td>
  </tr>
  <tr>
    <td class="tg-k9ij"><p> &#128309 &#128309 &#128309 &#128309 </p></td>
    <td class="tg-k9ij">4 x 0 x 4 = 0</td>
  </tr>
</table>

</center>

Accumulation d'évidence
========================================================
incremental: true
type: lineheight

Dans le cas précédent, nous avons considéré que toutes les hypothèses étaient équiprobables a priori (*principe d'indifférence*). Cependant, on pourrait avoir de l'information a priori, provenant de nos connaissances (des particularités des sacs de billes par exemple) ou de données antérieures.

Imaginons que nous tirions une nouvelle bille du sac, comment incorporer cette nouvelle donnée ?

Accumulation d'évidence
========================================================
incremental: false
type: lineheight

Il suffit d'appliquer la même stratégie que précédemment, et de mettre à jour le dernier compte en le multipliant par ces nouvelles données. *Yesterday's posterior is today's prior* (Lindley, 2000).

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:30px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:30px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothèse</th>
    <th class="tg-gq9a"><p>Façons de produire &#128309</p></th>
    <th class="tg-baqh">Compte précédent</th>
    <th class="tg-baqh">Nouveau compte</th>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#9898 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 0 = 0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">1</td>
    <td class="tg-baqh">3</td>
    <td class="tg-baqh">3 x 1 = 3</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">2</td>
    <td class="tg-baqh">8</td>
    <td class="tg-baqh">8 x 2 = 16</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#128309 &#9898 </p></td>
    <td class="tg-gq9a">3</td>
    <td class="tg-baqh">9</td>
    <td class="tg-baqh">9 x 3 = 27</td>
  </tr>
  <tr>
    <td class="tg-k9ij"><p> &#128309 &#128309 &#128309 &#128309 </p></td>
    <td class="tg-k9ij">4</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 4 = 0</td>
  </tr>
</table>

</center>

Incorporer un prior
========================================================
incremental: false
type: lineheight

Supposons maintenant qu'un employé de l'usine de fabrication des billes nous dise que les billes bleues sont rares... Cet employé nous dit que pour chaque sac contenant 3 billes bleues, ils fabriquent deux sacs en contenant seulement deux, et trois sacs en contenant seulement une. Il nous apprend également que tous les sacs contiennent au moins une bille bleue et une bille blanche.

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:30px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:30px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothèse</th>
    <th class="tg-gq9a"><p>Compte précédent</p></th>
    <th class="tg-baqh">Prior usine</th>
    <th class="tg-baqh">Nouveau compte</th>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#9898 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 0 = 0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">3</td>
    <td class="tg-baqh">3</td>
    <td class="tg-baqh">3 x 3 = 9</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">16</td>
    <td class="tg-baqh">2</td>
    <td class="tg-baqh">16 x 2 = 32</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#128309 &#9898 </p></td>
    <td class="tg-gq9a">27</td>
    <td class="tg-baqh">1</td>
    <td class="tg-baqh">27 x 1 = 27</td>
  </tr>
  <tr>
    <td class="tg-k9ij"><p> &#128309 &#128309 &#128309 &#128309 </p></td>
    <td class="tg-k9ij">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 0 = 0</td>
  </tr>
</table>

</center>

Des énumérations aux probabilités
========================================================
incremental: false
type: lineheight

La plausiblité d'une hypothèse après avoir observé certaines données est proportionnelle au nombre de façons qu'a cette hypothèse de produire les données observées, multiplié par sa plausibilité a priori.

$$p(hypothesis|data)\propto p(data|hypothesis) \times p(hypothesis)$$

Pour passer des *plausibilités* aux *probabilités*, il suffit de standardiser ces plausibilités pour que la somme des plausibilités de toutes les hypothèses possibles soit égale à $1$.

$$p(hypothesis|data) = \frac{p(data|hypothesis) \times p(hypothesis)}{sum\ of\ products}$$

Des énumérations aux probabilités
========================================================
incremental: true
type: lineheight

Définissons $p$ comme la proportion de billes bleues.

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:20px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:20px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothèse</th>
    <th class="tg-gq9a">p</th>
    <th class="tg-baqh">Manières de produire les données</th>
    <th class="tg-baqh">Plausibilité</th>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#9898 &#9898 &#9898 &#9898 </td>
    <td class="tg-gq9a">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#128309 &#9898 &#9898 &#9898 </td>
    <td class="tg-gq9a">0.25</td>
    <td class="tg-baqh">3</td>
    <td class="tg-baqh">0.15</td>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#128309 &#128309 &#9898 &#9898 </td>
    <td class="tg-gq9a">0.5</td>
    <td class="tg-baqh">8</td>
    <td class="tg-baqh">0.40</td>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#128309 &#128309 &#128309 &#9898 </td>
    <td class="tg-gq9a">0.75</td>
    <td class="tg-baqh">9</td>
    <td class="tg-baqh">0.45</td>
  </tr>
  <tr>
    <td class="tg-k9ij"> &#128309 &#128309 &#128309 &#128309 </td>
    <td class="tg-k9ij">1</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0</td>
  </tr>
</table>

</center>

```{r, echo = TRUE, eval = TRUE}
ways <- c(0, 3, 8, 9, 0)
ways / sum(ways)
```

Notations, terminologie
========================================================
incremental: false
type: lineheight

- $\theta$ un paramètre ou vecteur de paramètres (e.g., la proportion de billes bleues)
- $\color{orangered}{p(x\vert \theta)}$ <span style="color:orangered"> la distribution de probabilité conditionnelle des données $x$ sachant le paramètre $\theta$. Une fois que la valeur de $x$ est connue, est vue comme la fonction de vraissemblance (*likelihood*) du paramètre $\theta$</span>
- $\color{steelblue}{p(\theta)}$ <span style="color:steelblue"> la distribution de probabilité a priori de $\theta$</span>
- $\color{purple}{p(\theta \vert x)}$ <span style="color:purple"> la distribution de probabilité a posteriori de $\theta$ (sachant $x$)</span>
- $\color{green}{p(x)}$ <span style="color:green"> la distribution de probabilité marginale de $x$ (sur $\theta$)</span>

<br>

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{\sum\limits_{\theta}p(x|\theta)p(\theta)}} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{\int\limits_{\theta}p(x|\theta)p(\theta)\mathrm{d}x}} \propto \color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}$$

Loi de probabilité, cas discret
========================================================
type: lineheight
incremental: true

Une fonction de masse (*probability mass function*, ou *PMF*) est une fonction qui attribue une probabilité à chaque valeur d'une variable aléatoire. Exemple de la distribution binomiale pour une pièce non biaisée ($\theta=0.5$).

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
coin <- dbinom(x = 0:10, size = 10, prob = 0.5)
barplot(coin, names.arg = 0:10, col = "steelblue", border = NA)
```

```{r eval = TRUE, echo = TRUE}
dbinom(x = 0:10, size = 10, prob = 0.5) %>% sum
```

Cas continu
========================================================
type: lineheight
incremental: true

Une densité de probabilité (*probability density function*, ou *PDF*), est une fonction qui permet de représenter une loi de probabilité sous forme d'intégrales.

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
ggplot(data.frame(x = c(0, 200) ), aes(x) ) +
        stat_function(
                fun = dnorm, args = list(mean = 100, sd = 15),
                col = "steelblue", lwd = 2) +
        theme_bw(base_size = 20) + xlab("QI") + ylab("")
```

```{r echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
integrate(dnorm, -Inf, Inf, mean = 100, sd = 15)
```

Aparté, qu'est-ce qu'une intégrale ?
========================================================
type: lineheight
incremental: true

Une intégrale correspond à la **surface** (aire géométrique) délimitée par la représentation graphique d'une fonction, *l'aire sous la courbe*. Une distribution est dite **impropre** si son intégrale n'est pas égale à un nombre fini (e.g., $+ \infty$), et **normalisée** si son intégrale est égale à 1.

```{r message = FALSE, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
cord.x <- c(90, seq(90, 96, 0.01), 96) 
cord.y <- c(0, dnorm(seq(90, 96, 0.01), 100, 15), 0) 

ggplot(data = data.frame(x = c(0, 200) ), aes(x) ) +
        stat_function(
                fun = dnorm, args = list(mean = 100, sd = 15),
                col = "steelblue", lwd = 2) +
        geom_polygon(
                data = data.frame(cord.x, cord.y),
                aes(cord.x, cord.y),
                fill = "steelblue") +
        theme_bw(base_size = 20) +
        xlab("QI") +
        ylab("")
```

Aparté, qu'est-ce qu'une intégrale ?
========================================================
type: lineheight
incremental: true

```{r message = FALSE, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
ggplot(data = data.frame(x = c(0, 200) ), aes(x) ) +
        stat_function(
                fun = dnorm, args = list(mean = 100, sd = 15),
                col = "steelblue", lwd = 2) +
        geom_polygon(
                data = data.frame(cord.x, cord.y),
                aes(cord.x, cord.y),
                fill = "steelblue") +
        theme_bw(base_size = 20) +
        xlab("QI") +
        ylab("")
```

L'intégrale de $f(x)$ sur l'intervalle [90 ; 96] vaut: $\int_{90}^{96} f(x) \ \mathrm{d}x = 0.142$.

```{r eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
integrate(dnorm, 90, 96, mean = 100, sd = 15)
```

Probabilité conjointe
========================================================
incremental: false
type: lineheight

```{r, message = FALSE, echo = TRUE} 
library(magrittr)

data(HairEyeColor) # data adapted from Snee (1974)

cont <- apply(HairEyeColor, c(1, 2), sum) %>% t 
cont <- round(cont / sum(cont), 2)
cont
```

Dans chaque cellule, on trouve la **probabilité conjointe** d'avoir telle couleur de cheveux **ET** telle couleur d'yeux, qui s'écrit $p(c,y)=p(y,c)$.

Probabilité marginale
========================================================
incremental: false
type: lineheight

```{r, echo = TRUE} 
cont2 <- cont %>% as.data.frame %>% mutate(marginal_eye = rowSums(cont) )
rownames(cont2) <- row.names(cont)
cont2
```

On peut aussi s'intéresser à la probabilité d'avoir des yeux bleus, de manière générale. Il s'agit de la probabilité **marginale** de l'événement *yeux bleus*, qui s'obtient par la somme de toutes les probabilités jointes impliquant l'évenement *yeux bleus*. Elle s'écrit $p(y)=\sum\limits_{c}p(y|c)p(c)$.

Probabilité marginale
========================================================
incremental: false
type: lineheight

```{r, echo = TRUE, eval = TRUE} 
cont3 <- rbind(cont2, colSums(cont2) )
rownames(cont3) <- c(row.names(cont2), "marginal_hair")
cont3
```

On peut bien entendu aussi s'intéresser aux probabilités des couleurs de cheveux, de manière générale. Elle s'écrit $p(c)=\sum\limits_{y}p(c|y)p(y)$.

Probabilité conditionnelle
========================================================
incremental: true
type: lineheight

```{r, echo = FALSE, eval = TRUE} 
cont3["Blue",]
```

On pourrait aussi s'intéresser à la probabilité qu'une personne ait les cheveux blonds, **sachant** qu'elle a les yeux bleus. Il s'agit d'une probabilité **conditionnelle**, et s'écrit $p(c|y)$. Cette probabilité conditionnelle peut se ré-écrire: $p(c|y)= \frac{p(c,y)}{p(y)}$.

```{r, echo = TRUE, eval = TRUE} 
cont3["Blue","Blond"] / cont3["Blue","marginal_eye"]  
```

Probabilité conditionnelle, règle du produit
========================================================
incremental: true
type: lineheight

On remarque dans le cas précédent que $p(blonds|bleus)$ **n'est pas nécessairement égal** à $p(bleus|blonds)$.

Autre exemple: la probabilité de mourir sachant qu'on a été attaqué par un requin n'est pas la même que la probabilité d'avoir été attaqué par un requin, sachant qu'on est mort.

De la même manière, $p(data|H_{0}) \neq p(H_{0}|data)$ -> [*"confusion of the inverse"*](https://en.wikipedia.org/wiki/Confusion_of_the_inverse).

A partir des axiomes de Kolmogorov (cf. début du cours), et des définitions précédentes des probabilités conjointes, marginales, et conditionnelles, découle la **règle du produit**:

$$p(a,b)=p(b)p(a|b)=p(a)p(b|a)$$

Dérivation du théorème de Bayes
========================================================
incremental: true
type: lineheight

$$p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$

$$p(y|x)p(x) = p(x|y)p(y)$$

$$p(y|x) = \dfrac{p(x|y)p(y)}{p(x)}$$

$$p(x|y) = \dfrac{p(y|x)p(x)}{p(y)}$$

$$p(hypothesis|data) = \frac{p(data|hypothesis) \times p(hypothesis)}{sum\ of\ products}$$

Exemple - Diagnostique médical (Gigerenzer, 2002)
========================================================
incremental: true
type: lineheight

- Chez les femmes âgées de 40-50 ans, sans antécédents familiaux et sans symptômes, la probabilité d'avoir un cancer du sein est de .008.

- Propriétés de la mammographie:
  - Si une femme a un cancer du sein, la probabilité d'avoir un résultat positif est de .90
  - Si une femme n'a pas de cancer du sein, la probabilité d'avoir un résultat positif est de .07

- Imaginons qu'une femme passe une mammographie, et que le test est positif. Que doit-on **inférer** ? Quelle est la probabilité que cette femme ait un cancer du sein ?

Logique du Maximum Likelihood
========================================================
incremental: true
type: lineheight

- Une approche générale de l'estimation de paramètre
- Les paramètres **gouvernent** les données, les données **dépendent** des paramètres
  - Sachant certaines valeurs des paramètres, nous pouvons calculer la **probabilité conditionelle** des données observées
  - Le résultat de la mammographie (i.e., les données) dépend de la présence / absence d'un cancer du sein (i.e., le paramètre)
- L'approche par *maximum likelihood* (ML) pose la question: "*Quelles sont les valeurs du paramètre qui rendent les données observées les plus probables ?*"

Maximum Likelihood
========================================================
incremental: true
type: lineheight

- Spécifier la probabilité conditionnelle des données $p(x|\theta)$
- Quand on le considère comme fonction de $\theta$, on parle de **likelihood**: $L(\theta|x)=p(x|\theta)$
- L'approche par ML consiste donc à maximiser cette fonction, en utilisant les valeurs (connues) de $x$

Probabilité conditionnelle
========================================================
incremental: true
type: lineheight

- Si une femme a un cancer du sein, la probabilité d'obtenir un résultat positif est de .90
  - $p(Mam=+|Cancer=+)=.90$ 
  - $p(Mam=-|Cancer=+)=.10$
  
- Si une femme n'a pas de cancer du sein, la probabilité d'obtenir un résultat positif est de .07
  - $p(Mam=+|Cancer=-)=.07$ 
  - $p(Mam=-|Cancer=-)=.93$

Diagnostique médical, maximum likelihood
========================================================
incremental: true
type: lineheight

- Une femme passe une mammographie, le résultat est positif...
  - $p(Mam=+|Cancer=+)=.90$ 
  - $p(Mam=+|Cancer=-)=.07$
  
- ML: quelle est la valeur de *Cancer* qui **maximise** $Mam=+$ ?
  - $p(Mam=+|Cancer=+)=.90$ 

========================================================

<div align = "center" style="border:none;">
<img src = "bayes.png" width = 600 height = 600>
</div>

<center>*Wait a minute...*</center>

Diagnostique médical, fréquences naturelles
========================================================
incremental: true
type: lineheight

- Considérons 1000 femmes âgées de 40 à 50 ans, sans antécédents familiaux et sans symptômes de cancer
  - 8 femmes sur 1000 ont un cancer
- On réalise une mammographie
  - Sur les 8 femmes ayant un cancer, 7 auront un résultat positif
  - Sur les 992 femmes restantes, 69 auront un résultat positif
- Une femme passe une mammographie, le résultat est positif
- Que devrait-on **inférer** ?

Diagnostique médical, fréquences naturelles
========================================================
incremental: true
type: lineheight

<div align = "center" style = "border:none;">
<img src = "diagram.png" width = 850 height = 500>
</div>

<br>

$$p(Cancer=+|Mam=+)=\frac{7}{7+69}=\frac{7}{76}\approx.09$$

Diagnostique médical, théorème de Bayes
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

$\color{steelblue}{p(\theta)}$ <span style="color:steelblue"> la probabilité *a priori* de $\theta$: tout ce qu'on sait de $\theta$ avant d'observer les données. Par exemple: $p(Cancer=+)=.008$ et $p(Cancer=-)=.992$.</span>

```{r, echo = TRUE}
prior <- c(0.008, 0.992)
```

Diagnostique médical, théorème de Bayes
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

$\color{orangered}{p(x\vert \theta)}$ <span style="color:orangered"> probabilité conditionnelle des données ($x$) sachant le paramètre ($\theta$), qu'on appelle aussi la *likelihood* (*ou fonction de vraissemblance*) du paramètre ($\theta$).</span>

```{r, echo = TRUE}
like <- rbind(c(0.9, 0.1), c(0.07, 0.93) ) %>% data.frame
colnames(like) <- c("Mam+", "Mam-")
rownames(like) <- c("Cancer+", "Cancer-")
like
```

Diagnostique médical, théorème de Bayes
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

<span style="color:green"> $p(x)$ la probabilité marginale de $x$ (sur $\theta$). Sert à normaliser la distribution.</span>

$$\color{green}{p(x)=\sum\limits_{\theta}p(x|\theta)p(\theta)}$$

```{r, echo = TRUE}
(marginal <- sum(like$"Mam+" * prior) )
```

Diagnostique médical, théorème de Bayes
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

$\color{purple}{p(\theta \vert x)}$ <span style="color:purple"> la probabilité a posteriori de $\theta$ sachant $x$, c'est à dire ce qu'on sait de $\theta$ après avoir pris connaissance de $x$.</span>

```{r, echo = TRUE}
(posterior <- (like$"Mam+" * prior ) / marginal )
```

L'inférence bayésienne comme mise à jour probabiliste des connaissances
========================================================
incremental: true
type: lineheight

Avant de passer le mammogramme, la probabilité qu'une femme tirée au sort ait un cancer du sein était de $p(Cancer)=.008$ (*prior*). Après un résultat positif, cette probabilité est devenue $p(Cancer|Mam+)=.09$ (*posterior*). Ces probabilités sont des expressions de nos *connaissances*. Après un mammogramme positif, on pense toujours que c'est "très improbable" d'avoir un cancer, mais cette probabilité a considérablement évolué relativement à "avant le test".

> A Bayesianly justifiable analysis is one that *treats known values as observed values of random variables, treats unknown values as unobserved random variables, and calculates the conditional distribution of unknowns given knowns and model specifications using Bayes’ theorem*. -- Rubin (1984)

Monty Hall
========================================================
incremental: true
type: lineheight

<br>

<div align="center">
<video width="1200" height="600" controls="controls">
<source src="montyhall.mp4" type="video/mp4">
</video>
<div>

Monty Hall
========================================================
incremental: false
type: lineheight

<div align = "center" style="border:none;">
<img src = "monty1.png" width = 600 height = 600>
</div>

<center> What would you do (intuitively) ? Then, analyse the situation using Bayes theorem. </center>

Monty Hall
========================================================
incremental: true
type: lineheight

This is actually a conditional probability problem... Let's define the following events:

D1: the game hosts opens door n°1 <br>
D2: the game hosts opens door n°2 <br>
D3: the game hosts opens door n°3 <br>

C1: the car is behing door n°1 <br>
C2: the car is behing door n°2 <br>
C3: the car is behing door n°3 <br>

If we choose door n°1 and the host chose door n°3 (*and given that he knows where is the car*), it follows that:

$p(D3|C1)=\dfrac{1}{2}$, $p(D3|C2)=1$, $p(D3|C3)=0$

Monty Hall
========================================================
incremental: true
type: lineheight

We know that $p(V3|P3)=0$, we want to know $p(V1|P3)$ and $p(V2|P3)$ in order to choose...

$p(V1|P3)=\dfrac{p(P3|V1) \times p(V1)}{p(P3)}=\dfrac{\dfrac{1}{2} \times \dfrac{1}{3}}{\dfrac{1}{2}}=\dfrac{1}{3}$

$p(V2|P3)=\dfrac{p(P3|V2) \times p(V2)}{p(P3)}=\dfrac{1 \times \dfrac{1}{3}}{\dfrac{1}{2}}=\dfrac{2}{3}$

Monty Hall
========================================================
incremental: true
type: lineheight

<div align = "center" style="border:none;">
<img src = "monty2.png" width = 600 height = 600>
</div>

Summary
========================================================
incremental: true
type: lineheight

Our probabilistic intuitions are usually very bad. Instead of relying on them to make decisions, it is safer to rely on logic (e.g., *modus ponens* or *moduls tollens*) and probabilistic rules (the sum rule, the product rule, Bayes' rule). These rules will conduct us to the most logic inference.

In other words: "Don't be clever" (McElreath, 2015).

```{r echo = FALSE, fig.align = "center", out.width = "600px"}
knitr::include_graphics("morale.gif")
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script>

for(i=0;i<$("section").length;i++) {
if(i==0) continue
$("section").eq(i).append("<p style='font-size:xx-large;position:fixed;right:200px;bottom:50px;'>" + i + "</p>")
}

</script>
