Introduction to Bayesian inference
========================================================
author: Ladislas Nalborczyk
date: Univ. Grenoble Alpes, CNRS, LPNC (France) • Ghent University (Belgium)
autosize: true
transition: none
width: 1600
height: 1000
css: css-file.css

Probabilistic interpretations
========================================================
incremental: true
type: lineheight

```{r setup, include = FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
```

<!-- For syntax highlighting -->
<link rel="stylesheet" href="github.css">

What is the probability...

* Of getting head when flipping a coin ?
* That I learn something new during this course ?

Do these two questions refer to the same concept of *probability* ?

```{r echo = FALSE, fig.align = "center", out.width = "600px"}
knitr::include_graphics("thinking.gif")
```

Probability axioms (Kolmogorov, 1933)
========================================================
incremental: true
type: lineheight

A probability is **a numerical value** assigned to an event $A$, this event being a possibility of an ensemble $\Omega$ (the ensemble of all possible events). Probabilities conform to the following axioms (the third axiom is only valid for mutually exclusive events):

+ **Non-negativity.** $P(A_{i}) \geq 0$
+ **Normalisation.** $P(\Omega) = 1$
+ **Additivity.** $P(A_{1}\cup A_{2}\cup \ldots) = \sum_{i=1}^{+\infty}P(A_{i})$

Classical (or theoretical) interpretation
========================================================
incremental: true
type: lineheight

$$P(even)=\frac{number\ of\ favorable\ cases}{number\ of\ possible\ cases}=\frac{3}{6}=\frac{1}{2}$$

Problem: this definition only applies to situations in which there is a finite ensemble of equiprobable events...

> What is the probability of raining tomorrow ?

> $$P(rain)=\frac{rain}{ \{ rain,\ non-rain \} }=\frac{1}{2}??$$

Frequentist (or empirical) interpretation
========================================================
incremental: true
type: lineheight

$$P(x)=\lim_{n_{t} \to \infty}\frac{n_{x}}{n_{t}}$$

Where $n_{x}$ is the number of events $x$ and $n_{t}$ the total number of trials. The **frequentist** interpretation postulates that, **in the long run** (when the number of trial approaches infinity), the relative frequency will eventually converge to what we call *probability*.

Consequence: the concept or *probability* only applies to collectives, and not to singular events.

Frequentist (or empirical) interpretation
========================================================
incremental: false
type: lineheight
    
```{r, message = FALSE, fig.width = 8, fig.heigth = 4, fig.align = "center"} 
library(tidyverse)

sample(c(0, 1), 500, replace = TRUE) %>%
        data.frame %>%
        mutate(x = seq_along(.), y = cumsum(.) / seq_along(.) ) %>%
        ggplot(aes(x = x, y = y), log = "y") +
        geom_line(lwd = 1) +
        geom_hline(yintercept = 0.5, lty = 3) +
        xlab("number of trials") +
        ylab("proportion of heads") +
        ylim(0, 1) +
        theme_bw(base_size = 20)
```

Problems...
========================================================
incremental: true
type: lineheight

- What is the reference class ? *What is the probability of me living until 80 years ?*

- What about non-repeatable events ? *What is the probability of learning something new during this course ?*

- How many trials are sufficient to obtain a good approximation of the *probability* ? A finite class of events of size $n$ can only produce relative frequencies of certain preciseness $1/n$.

Propensity interpretation
========================================================
incremental: true
type: lineheight

The frequentist properties (in the long run) of objects (e.g., a coin) might be produced by intrinsic properties of these objects. For instance, a biased coin will produce a biased relative frequency **because** of its intrinsic bias.

The propensity interpretation postulates that these intrinsic properties define what we call as probability. In other words, the probability of an event = its intrinsic properties.

This interpretation will usually agree with the frequentist interpretation, but it shifts the locus of probability from the external world (the observable relative frequency in the long run) to internal properties of objects.

Consequence: these properties are properties that are intrinsic to objects... The propensity interpretation then allows to talk about the probability of single events.

Logical interpretation
========================================================
incremental: true
type: lineheight

<center>There are 10 students in this room</center>
<center>9 students wear a <font color = "green">green</font> t-shirt</center>
<center>1 student wears a <font color = "red">red</font> t-shirt</center>
<center>Let's say 1 person is picked at random...</center>

<hr width = "75%%" size = "2" align = "center" noshade>

> <center>Conclusion 1: this student wears a t-shirt &#10004; </center>

<hr width = "75%%" size = "1" align = "center" noshade>

> <center>Conclusion 2: this student wears a <font color = "green">green t-shirt</font> &#10007; </center>

<hr width = "75%%" size = "1" align = "center" noshade>

> <center>Conclusion 3: this student wears a <font color = "red">red t-shirt</font> &#10007; </center>

Logical interpretation
========================================================
incremental: false
type: lineheight

The logical interpretation of probability aims at generalising the formal logic (true / false) to the probabilist world. The probability then represents the degree of *logical support* of a conclusion, in relation to a set of premises ([Keynes, 1921](https://archive.org/details/treatiseonprobab007528mbp); [Carnap, 1950](http://fitelson.org/confirmation/carnap_logical_foundations_of_probability.pdf)).

Consequence: every probability is **conditional**.

Bayesian interpretation
========================================================
incremental: true
type: lineheight

The probability is a way of **quantifying uncertainty**. A certain event then has a probability of 1 and an impossible event has a probability of 0.

This interpretation eliminates the need for a collective of infinite repretition of an event...

> *So to assign equal probabilities to two events is not in any way an assertion that they must occur equally often in any random experiment [...], it is only a formal way of saying I don’t know* ([Jaynes, 1986](http://bayes.wustl.edu/etj/articles/general.background.pdf)).

Probabilistic interpretations
========================================================
incremental: false
type: lineheight

+ Classic interpretation (Laplace, Bernouilli, Leibniz)
+ **Frequentist interpretation** (Venn, Reichenbach, von Mises)
+ Propensity interpretation (Popper, Miller)
+ Logical interpretation (Keynes, Carnap)
+ **Bayesian interpretation** (Jeffreys, de Finetti, Savage)

*[Click for more details...](http://plato.stanford.edu/entries/probability-interpret/)*

Probabilistic interpretations - summary
========================================================
incremental: true
type: lineheight

> **Epistemic probability**
- every probability is conditional on some available information (e.g., premises or data)
- probabilities as a way of quantifying uncertainty
- logical interpretation, Bayesian interpretation

<hr style="height:20pt; visibility:hidden;" />

> **Physical probability**
- probabilities depend on a state of the world, on physical characteristics. They are independent of knowledge or uncertainty.
- classic interpretation, frequentist interpretation</center>

========================================================
type: black

<img src="pill.jpg" height="1000px" width="1600px" />

A piece of formal logic
========================================================
type: lineheight
incremental: true

<div align = "center" style="float: bottom;">
<img src = "penguin.jpg" width = 600 height = 600>
</div>

A piece of formal logic
========================================================
type: lineheight
incremental: true

**Example n°1**

- If a suspect is lying, he will sweat. He is sweating.
- So, he is lying.

**Example n°2**

- If a suspect is sweating, he is lying. He is not sweating.
- So, he is not lying.

**Example n°3**

- All liars are sweating. This suspect is not sweating.
- So, he is not a liar.

Invalid inferences
========================================================
type: lineheight
incremental: true

- *affirming the consequent*: $\dfrac{A \Rightarrow B, \ B}{A}$

- If it rains, the ground is wet (A implies B). The ground is wet (B). Then, it rained (A).

- *denying the antecedent*: $\dfrac{A \Rightarrow B, \ \neg A}{\neg B}$

- If it rains, the ground is wet (A implies B). It did not rain (non A). Then, the ground is not wet (non A).

```{r echo = FALSE, fig.align = "center", out.width = "600px"}
knitr::include_graphics("trump.gif")
```

Valid inferences
========================================================
type: lineheight
incremental: true

- *modus ponens*: $\dfrac{A \Rightarrow B, \ A}{B}$

- If today is Monday, then John will go to work (A implies B). Today is Monday (A). Then John will go to work (B).

- *modus tollens*: $\dfrac{A \Rightarrow B, \ \neg B}{\neg A}$

- If my dog feels the presence of an intruder, he will bark (A implies B). My dog did not bark (not B). Then, he did not feel the presence of an intruder (not A).

```{r echo = FALSE, fig.align = "center", out.width = "400px"}
knitr::include_graphics("good.gif")
```

Logic, frequentism and probabilistic reasoning
========================================================
incremental: true
type: lineheight

The *modus tollens* is one of the strongest rule of inference in logic. It works perfectly well in science when we deal with hypotheses of the following form: *If $H_{0}$ is true, then we should not observe $x$. We observed $x$. Then, $H_{0}$ is false*.

BUT, most of the time, we deal with *continuous*, *probabilistic* hypotheses...

The Fisherian inference (induction) is of the form: *If $H_{0}$ is true, then we should PROBABLY not observe $x$. We observed $x$. Then, $H_{0}$ is PROBABLY false*.

But this argument is invalid. The *modus tollens* does not apply to probabilistic statements (e.g., [Pollard & Richardson, 1987](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.505.9968&rep=rep1&type=pdf); [Rouder, Morey, Verhagen, Province, & Wagenmakers, 2016](http://www.ejwagenmakers.com/2016/RouderEtAl2016FreeLunch.pdf)).

Let's take an example: *If Jane is an American, then it is unlikely that she is a U.S. Congressperson. Jane is a U.S. Congressperson. Then, Jane is probably not an American...*

The failure of strict falsificationism
========================================================
type: lineheight
incremental: true

*Naive popperism*: science is progressing through falsification, then data analysis sould aim for falsification (e.g., using NHST). But...

- (theoretical hypotheses) are not equivalent to (statistical) models, there is no one-to-one mapping between the two

> *Models are devices that connect theories to data. A model is an instanciation of a theory as a set of probabilistic statements* ([Rouder et al., 2016](https://www.collabra.org/articles/10.1525/collabra.28/)).

- our hypothese are mostly continuous, probabilistic
- observation errors
- falsification concerns the demarcation problem
- science is a social technology, falsification is consensual, it cannot be logic (see also [Meehl, 1990](https://www.jstor.org/stable/1448781?seq=1#page_scan_tab_contents))

We need a plan
========================================================
type: lineheight
incremental: true

The tools we are going to use:

- *Bayesian data analysis*: using probability theory to describe uncertainty
- ~~*Multilevel modelling*: describing and spreading uncertainty at multiple levels~~
- ~~*Model comparison approach*: instead of trying to falsify a null model, comparing interesting and theoretically sound models (e.g., using AIC or WAIC)~~

Exercice - The marbles bag
========================================================
type: lineheight
incremental: true

Let's say we have a bag and that it contains four marbles. These marbles come in two colors: blue and white. We know that there are four marbles in the bag, but we don't know how many are of each color.

We know that there are five possibilities (than we will call our *hypotheses*):

> <p align = "center"> &#9898 &#9898 &#9898 &#9898</p>
> <p align = "center"> &#128309 &#9898 &#9898 &#9898</p>
> <p align = "center">&#128309 &#128309 &#9898 &#9898</p>
> <p align = "center">&#128309 &#128309 &#128309 &#9898</p>
> <p align = "center">&#128309 &#128309 &#128309 &#128309</p>

Exercice - The marbles bag
========================================================
type: lineheight
incremental: true

Our goal is to determine which hypothesis is the most plausible, given some **evidence** about the content of the bag. To obtain evidence, we can pull some marbles from the bag (with replacement). We did it three times, and obtained the following sequence:

<p align = "center">&#128309 &#9898 &#128309</p>

This sequence represents some evidence about the content of the bag, in other words, our data. From there, what **inference** can we reasonably make about the content of the bag ? In other words, what can we say about the relative plausibility of each hypothesis ?

Counting possibilities
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothesis: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; data: &#128309 </p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
library(rethinking)
source("forking_data_McElreath.R")

dat <- c(1)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 1
    )
```

Counting possibilities
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothesis: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; data: &#128309 &#9898</p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
dat <- c(1, 0)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 1
    )
```

Counting possibilities
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothesis: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; data: &#128309 &#9898 &#128309</p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
dat <- c(1, 0, 1)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 1
    )
```

Counting possibilities
========================================================
type: lineheight
incremental: false

<p align = "center"> hypothesis: &#128309 &#9898 &#9898 &#9898 &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp; data: &#128309 &#9898 &#128309 </p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
dat <- c(1, 0, 1)
arc <- c(0, pi)

garden(
    arc = arc,
    possibilities = c(0, 0, 0, 1),
    data = dat,
    hedge = 0.05,
    ring_dist = ring_dist,
    alpha.fade = 0.3
    )
```

Counting possibilities
========================================================
incremental: false
type: lineheight

Under this hypothesis, $3$ paths (out of $4^{3}=64$) are consistent with the data. What about the other hypotheses ?

<p align = "center"> &#9898 &#9898 &#9898 &#128309 &emsp;&emsp;&emsp;&emsp; &#9898 &#128309 &#128309 &#128309 &emsp;&emsp;&emsp;&emsp; &#9898 &#9898 &#128309 &#128309 </p>

```{r, echo = FALSE, eval = TRUE, fig.height = 10, fig.width = 10, fig.align = "center"}
dat <- c(1, 0, 1)
ac <- c(1.2, 0.9, 0.6)

arc <- c( pi / 2, pi / 2 + (2 / 3) * pi)

garden(
    arc = arc,
    possibilities = c(1, 0, 0, 0),
    data = dat,
    hedge = 0.05,
    adj.cex = ac) 

arc <- c(arc[2], arc[2] + (2 / 3) * pi)

garden(
    arc = arc,
    possibilities = c(1, 1, 0, 0),
    data = dat,
    hedge = 0.05,
    newplot = FALSE,
    adj.cex = ac)

arc <- c(arc[2], arc[2] + (2 / 3) * pi)

garden(
    arc = arc,
    possibilities = c(1, 1, 1, 0),
    data = dat,
    hedge = 0.05,
    newplot = FALSE,
    adj.cex = ac)

line.polar(c(0, 2), pi / 2, lwd = 1)
line.polar(c(0, 2), pi / 2 + (2 / 3) * pi, lwd = 1)
line.polar(c(0, 2), pi / 2 + 2 * (2 / 3) * pi, lwd = 1)
```

Model comparison
========================================================
incremental: false
type: lineheight

<p align = "center"> &#128309 &#128309 &#128309 &#9898 </p>

Given our data, this hypothesis is the most *probable* because it's the one that **maximise** the possible ways of obtaining these data.

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:30px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:30px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothesis</th>
    <th class="tg-gq9a">Ways to produce the data</th>
  </tr>
  <tr>
    <td class="tg-gq9a"> <p> &#9898 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a"> 0 x 4 x 0 = 0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">1 x 3 x 1 = 3</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">2 x 2 x 2 = 8 </td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#128309 &#9898 </p></td>
    <td class="tg-gq9a">3 x 1 x 3 = 9</td>
  </tr>
  <tr>
    <td class="tg-k9ij"><p> &#128309 &#128309 &#128309 &#128309 </p></td>
    <td class="tg-k9ij">4 x 0 x 4 = 0</td>
  </tr>
</table>

</center>

Accumulating evidence
========================================================
incremental: true
type: lineheight

We previously considered that all hypotheses were a priori equiprobable ([principle of indifference](https://en.wikipedia.org/wiki/Principle_of_indifference)).

However, we could have a priori knowledge, either coming from our beliefs, from previous data or from knowledge about usual bags of this kind.

Let's say we pull another marble out of the bag. How can we incorporate this new information into our previous analysis ?

Accumulating evidence
========================================================
incremental: false
type: lineheight

We just have to apply the same strategy as previously. We can upate our previous counts by multiplying it by the new information. *Yesterday's posterior is today's prior* ([Lindley, 2000](http://www.phil.vt.edu/dmayo/personal_website/Lindley_Philosophy_of_Statistics.pdf)).

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:30px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:30px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothesis</th>
    <th class="tg-gq9a"><p>Ways to produce &#128309</p></th>
    <th class="tg-baqh">Previous counts</th>
    <th class="tg-baqh">New count</th>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#9898 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 0 = 0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">1</td>
    <td class="tg-baqh">3</td>
    <td class="tg-baqh">3 x 1 = 3</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">2</td>
    <td class="tg-baqh">8</td>
    <td class="tg-baqh">8 x 2 = 16</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#128309 &#9898 </p></td>
    <td class="tg-gq9a">3</td>
    <td class="tg-baqh">9</td>
    <td class="tg-baqh">9 x 3 = 27</td>
  </tr>
  <tr>
    <td class="tg-k9ij"><p> &#128309 &#128309 &#128309 &#128309 </p></td>
    <td class="tg-k9ij">4</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 4 = 0</td>
  </tr>
</table>

</center>

Using prior information
========================================================
incremental: false
type: lineheight

Suppose that someone from the marble factory told us that blue marbles are rare. For every bag containing three blue marbles, they make two bags that only contain two blue marbles, and three bags that only contain one blue marble. He also told us that every bag has at least one marble of each color.

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:30px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:30px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothesis</th>
    <th class="tg-gq9a"><p>Previous counts</p></th>
    <th class="tg-baqh">Factory count</th>
    <th class="tg-baqh">New count</th>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#9898 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 0 = 0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#9898 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">3</td>
    <td class="tg-baqh">3</td>
    <td class="tg-baqh">3 x 3 = 9</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#9898 &#9898 </p></td>
    <td class="tg-gq9a">16</td>
    <td class="tg-baqh">2</td>
    <td class="tg-baqh">16 x 2 = 32</td>
  </tr>
  <tr>
    <td class="tg-gq9a"><p> &#128309 &#128309 &#128309 &#9898 </p></td>
    <td class="tg-gq9a">27</td>
    <td class="tg-baqh">1</td>
    <td class="tg-baqh">27 x 1 = 27</td>
  </tr>
  <tr>
    <td class="tg-k9ij"><p> &#128309 &#128309 &#128309 &#128309 </p></td>
    <td class="tg-k9ij">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0 x 0 = 0</td>
  </tr>
</table>

</center>

From counts to probability
========================================================
incremental: true
type: lineheight

The plausibility of an hypothesis after seeing some data is proportional to the number of way this hypothesis can "produce" the data, muplitplied by its *a priori* plausibility.

$$p(hypothesis|data)\propto p(data|hypothesis) \times p(hypothesis)$$

Then, we construct probabilities by standardising thee plausibilities so that the sum of all plausibilities is equal to 1.

$$p(hypothesis|data) = \frac{p(data|hypothesis) \times p(hypothesis)}{sum\ of\ products}$$

From counts to probability
========================================================
incremental: true
type: lineheight

Let's define $p$ as the proportion of blue marbles.

<center>

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{font-family:Arial, sans-serif;font-size:20px;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg th{font-family:Arial, sans-serif;font-size:20px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:0px;overflow:hidden;word-break:normal;border-top-width:1px;border-bottom-width:1px;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-gq9a{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center}
.tg .tg-k9ij{font-family:"Lucida Sans Unicode", "Lucida Grande", sans-serif !important;;text-align:center;vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-gq9a">Hypothesis</th>
    <th class="tg-gq9a">p</th>
    <th class="tg-baqh">Ways to produce the data</th>
    <th class="tg-baqh">Plausibility</th>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#9898 &#9898 &#9898 &#9898 </td>
    <td class="tg-gq9a">0</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0</td>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#128309 &#9898 &#9898 &#9898 </td>
    <td class="tg-gq9a">0.25</td>
    <td class="tg-baqh">3</td>
    <td class="tg-baqh">0.15</td>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#128309 &#128309 &#9898 &#9898 </td>
    <td class="tg-gq9a">0.5</td>
    <td class="tg-baqh">8</td>
    <td class="tg-baqh">0.40</td>
  </tr>
  <tr>
    <td class="tg-gq9a"> &#128309 &#128309 &#128309 &#9898 </td>
    <td class="tg-gq9a">0.75</td>
    <td class="tg-baqh">9</td>
    <td class="tg-baqh">0.45</td>
  </tr>
  <tr>
    <td class="tg-k9ij"> &#128309 &#128309 &#128309 &#128309 </td>
    <td class="tg-k9ij">1</td>
    <td class="tg-baqh">0</td>
    <td class="tg-baqh">0</td>
  </tr>
</table>

</center>

```{r, echo = TRUE, eval = TRUE}
ways <- c(0, 3, 8, 9, 0)
ways / sum(ways)
```

Notations, terminology
========================================================
incremental: false
type: lineheight

- $\theta$ a parameter or vector of parameters (e.g., the proportion of blue marbles)
- $\color{orangered}{p(x\vert \theta)}$ <span style="color:orangered"> the conditional probability distribution of the data $x$ given $\theta$. Once we know $x$, can be seen as the likelihood function of $\theta$</span>
- $\color{steelblue}{p(\theta)}$ <span style="color:steelblue"> the a priori probability distribution of $\theta$</span>
- $\color{purple}{p(\theta \vert x)}$ <span style="color:purple"> the a posteriori probability distribution of $\theta$ (given $x$)</span>
- $\color{green}{p(x)}$ <span style="color:green"> the marginal probability of $x$ (over $\theta$)</span>

<br>

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{\sum\limits_{\theta}p(x|\theta)p(\theta)}} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{\int\limits_{\theta}p(x|\theta)p(\theta)\mathrm{d}x}} \propto \color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}$$

Bayesian inference
========================================================
type: lineheight
incremental: true

For each problem, we will follow these three steps:

- Building the model (the history of the data): likelihood + priors
- Updating the model with the information contained in the data, compute the posterior probability
- Evaluate the model, its fit, its assumptions, summarising the results, readjusting the model

> *Bayesian inference is really just counting and comparing of possibilities [...] in order to make good inference about what actually happened, it helps to consider everything that could have happened.* ([McElreath, 2015](http://xcelab.net/rm/statistical-rethinking/)).

Reminders of probability...
========================================================
type: center
incremental: false


Probability distributions
========================================================
type: lineheight
incremental: true

A **probability mass function** (PMF) is a function that gives the probability that a discrete random variable is exactly equal to some value. Let's consider as an example the binomial distribution of a non biased coin ($\theta = 0.5$) for ten flips.

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
coin <- dbinom(x = 0:10, size = 10, prob = 0.5)
barplot(coin, names.arg = 0:10, border = NA, axes = FALSE, cex.names = 2)
```

```{r eval = TRUE, echo = TRUE}
dbinom(x = 0:10, size = 10, prob = 0.5) %>% sum
```

Probability distributions
========================================================
type: lineheight
incremental: true

A **probability density function** (PDF), or *density*, is a function that is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable’s PDF over that range.

```{r echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
data.frame(x = c(0, 200) ) %>%
    ggplot(aes(x) ) +
    stat_function(
        fun = dnorm,
        args = list(mean = 100, sd = 15),
        lwd = 2
        ) +
    theme_bw(base_size = 20) +
    xlab("QI") + ylab("")
```

```{r echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
integrate(dnorm, -Inf, Inf, mean = 100, sd = 15)
```

But... what is an integral ?
========================================================
type: lineheight
incremental: true

An integral corresponds to the **surface** delimited by the graphical representation of a function. In other words, it corresponds to the area under the curve. A distribution is said to be improper if its integral is not equal to a finite number (e.g., $+ \infty$), and normalised (or proper) if its integral is equal to 1.

```{r message = FALSE, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
cord.x <- c(90, seq(90, 96, 0.01), 96) 
cord.y <- c(0, dnorm(seq(90, 96, 0.01), 100, 15), 0) 

data.frame(x = c(0, 200) ) %>%
    ggplot(aes(x) ) +
    stat_function(
        fun = dnorm,
        args = list(mean = 100, sd = 15),
        lwd = 2
        ) +
    geom_polygon(
        data = data.frame(cord.x, cord.y),
        aes(cord.x, cord.y)
        ) +
    theme_bw(base_size = 20) +
    xlab("QI") + ylab("")
```

But... what is an integral ?
========================================================
type: lineheight
incremental: true

```{r message = FALSE, echo = FALSE, fig.align = "center", fig.width = 8, fig.height = 6}
data.frame(x = c(0, 200) ) %>%
    ggplot(aes(x) ) +
    stat_function(
        fun = dnorm,
        args = list(mean = 100, sd = 15),
        lwd = 2
        ) +
    geom_polygon(
        data = data.frame(cord.x, cord.y),
        aes(cord.x, cord.y)
        ) +
    theme_bw(base_size = 20) +
    xlab("QI") + ylab("")
```

The integral of $f(x)$ on the [90 ; 96] interval is equal to $\int_{90}^{96} f(x) \ \mathrm{d}x = 0.142$.

```{r eval = TRUE, echo = TRUE, fig.align = "center", fig.width = 8, fig.height = 6}
integrate(dnorm, 90, 96, mean = 100, sd = 15)
```

Joint probability
========================================================
incremental: false
type: lineheight

```{r, message = FALSE, echo = TRUE} 
data(HairEyeColor) # data adapted from Snee (1974)

cont <- apply(HairEyeColor, c(1, 2), sum) %>% t 
cont <- round(cont / sum(cont), 2)
cont
```

In each cell we find the **joint probability** to have both a specific hair colour AND an eye colour. The joint probability is written as: $p(h, e) = p(e, h)$.

Marginal probability
========================================================
incremental: false
type: lineheight

```{r, echo = TRUE} 
cont2 <- cont %>% as.data.frame %>% mutate(marginal_eye = rowSums(cont) )
rownames(cont2) <- row.names(cont)
cont2
```

We could also be interested in the probability of having blue eyes in average. We call it the **marginal probability** of the event *blues eyes*. It is obtained by summing all the joint probabilities involving the event *blue eyes*: $p(e) = \sum\limits_{h}p(e|h)p(h)$.

Marginal probability
========================================================
incremental: false
type: lineheight

```{r, echo = TRUE, eval = TRUE} 
cont3 <- rbind(cont2, colSums(cont2) )
rownames(cont3) <- c(row.names(cont2), "marginal_hair")
cont3
```

Similary, we can compute the marginal probaility of having a particular hair colour, as $p(h) = \sum\limits_{e}p(h|e)p(e)$.

Conditional probability
========================================================
incremental: true
type: lineheight

We could also be interested in the probability of a person having blond eyes, **given that** she has blue eyes.

```{r, echo = FALSE, eval = TRUE} 
cont3["Blue", ]
```

This is a **conditional probability** $p(h|e)$. This conditional probability can be rewrtitten as $p(h|e) =  \frac{p(h, e)}{p(e)}$.

```{r, echo = TRUE, eval = TRUE} 
cont3["Blue", "Blond"] / cont3["Blue", "marginal_eye"]  
```

Conditional probability, product rule
========================================================
incremental: true
type: lineheight

We notice that, in the previous situation, $p(hairs = blond|eyes = blue)$ **is not necessarily equal to** $p(eyes = blue|hairs = blond)$.

As an example, the probability of dying given that we have been attacked by a white shark is not the same as the probability of having been attacked by a shark, given that we are dead.

In a similar way, $p(data|H_{0}) \neq p(H_{0}|data)$ -> [*"confusion of the inverse"*](https://en.wikipedia.org/wiki/Confusion_of_the_inverse).

From the Kolmogorov axioms (cf beginning of this course), and the definitions of the previous slides, we can derive the **product rule**:

$$p(a, b) = p(b)p(a|b) = p(a)p(b|a)$$

Deriving Bayes theorem
========================================================
incremental: true
type: lineheight

$$p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$

$$p(y|x)p(x) = p(x|y)p(y)$$

$$p(y|x) = \dfrac{p(x|y)p(y)}{p(x)}$$

$$p(x|y) = \dfrac{p(y|x)p(x)}{p(y)}$$

$$p(hypothesis|data) = \frac{p(data|hypothesis) \times p(hypothesis)}{sum\ of\ products}$$

Example
========================================================
type: center
incremental: false


Medical diagnosis (Gigerenzer, 2002)
========================================================
incremental: true
type: lineheight

- In women between 40 and 50 years, without family history and without symptoms, the probability of having a breast cancer is .008.

- Properties of the mammography
  - If a woman has a breast cancer, the probability of obtaining a positive result is .90 (sensitivity)
  - If a woman does not have a breast cancer, the probability of obtaining a positive result is .07 (1-specificity)

- Let's ray a randomly picked woman passes the test and gets a positive result. What should we infer ? What is the probability of this woman having a breast cancer ?

Maximum Likelihood
========================================================
incremental: true
type: lineheight

- The maximum likelihood approach is a general approach of parameter estimation

- One of the main idea is that the parameters have a fixed but unknown value, and that data depend on these parameters

- Given certain parameters values, we can compute the **conditional probability** of the data

- The maximum likelihood (ML) approach ask the following question: what are the parameters values that maximise, i.e., that make the observed data the most probable ?

- When we consider it as a function of $\theta$, we talk about the **likelihood** function: $L(\theta|x) = p(x|\theta)$

- The ML approach then consists in maximising this function (i.e., finding its maximum) by using the known values of $x$ (i.e., the data)

Conditional probabilities...
========================================================
incremental: true
type: lineheight

- If a woman has a breast cancer, the probability of obtaining a positive result is .90
  - $p(mam=+|cancer=+)=.90$ 
  - $p(mam=-|cancer=+)=.10$
  
- If a woman does not have a breast cancer, the probability of obtaining a positive result is .07
  - $p(mam=+|cancer=-)=.07$ 
  - $p(mam=-|cancer=-)=.93$

Maximum Likelihood
========================================================
incremental: true
type: lineheight

- A randomly picked woman gets a positive result...
  - $p(mam=+|cancer=+)=.90$ 
  - $p(mam=+|cancer=-)=.07$
  
- The Maximum Likelihood approach asks the following quesiton: what is the value of *cancer* that **maximises** the probability of having a positive result (i.e., $Mam = +$) ?
  - $p(mam=+|cancer=+)=.90$
  - ~~$p(mam=+|cancer=-)=.07$~~

========================================================

<div align = "center" style="border:none;">
<img src = "bayes.png" width = 600 height = 600>
</div>

<center>*Wait a minute...*</center>

Natural frequencies
========================================================
incremental: true
type: lineheight

Let's summarise the situation.

- Let's consider 1000 women aged between 40 and 50 years, without family history of cancer or any syptoms
  - 8 woment out of 1000 have a cancer

- We do a mammography
  - On the 8 women having a cancer, we can expect 7 of them to have a positive result
  - On the 992 other women, 69 will have a positive result
  
- A randomly picked womand gets a positive result...

- What should we infer ?

Natural frequencies
========================================================
incremental: true
type: lineheight

<div align = "center" style = "border:none;">
<img src = "diagram2.png" width = 850 height = 500>
</div>

<br>

$$p(cancer = + | mam = +) = \frac{7}{7 + 69} = \frac{7}{76} \approx .09$$

Using Bayes theorem
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

$\color{steelblue}{p(\theta)}$ <span style="color:steelblue"> is the prior probability of $\theta$: everything we know about $\theta$ before to observe any data. For instance: $p(cancer = +) = .008$ and $p(cancer = -) = .992$.</span>

```{r, echo = TRUE}
prior <- c(0.008, 0.992)
```

Using Bayes theorem
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

$\color{orangered}{p(x\vert \theta)}$ <span style="color:orangered"> is the conditional probability of the data $x$ given the parameter $\theta$,  that we also call the likelihood function of $\theta$.</span>

```{r, echo = TRUE}
like <- rbind(c(0.9, 0.1), c(0.07, 0.93) ) %>% data.frame
colnames(like) <- c("mam+", "mam-")
rownames(like) <- c("cancer+", "cancer-")
like
```

Using Bayes theorem
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

<span style="color:green"> $p(x)$ is the marginal probability of the data $x$ (over $\theta$). Aims to normalise the distribution.</span>

$$\color{green}{p(x) = \sum\limits_{\theta}p(x|\theta)p(\theta)}$$

```{r, echo = TRUE}
(marginal <- sum(like$"mam+" * prior) )
```

Using Bayes theorem
========================================================
incremental: true
type: lineheight

$$\color{purple}{p(\theta \vert x)} = \dfrac{\color{orangered}{p(x\vert \theta)} \color{steelblue}{p(\theta)}}{\color{green}{p(x)}}$$

$\color{purple}{p(\theta \vert x)}$ <span style="color:purple"> is the posterior probability of $\theta$ given the data $x$. It represents everything we know about $\theta$ after observing the data $x$.</span>

```{r, echo = TRUE}
(posterior <- (like$"mam+" * prior ) / marginal )
```

Bayesian inference as knowledge updating
========================================================
incremental: true
type: lineheight

Before the mammography, the probability of a random woman having a breast cancer was $p(cancer)=.008$ (*prior*). After a positive result, this probability became $p(cancer|mam+) = .09$ (*posterior*).

These probabilities reflect our knowledge and our uncertainty. After a positive mammography, we still think that is it highly improbable to have a cancer, but our initial prior probability has considerably increased relatively to "before the test".

> *A Bayesianly justifiable analysis is one that treats known values as observed values of random variables, treats unknown values as unobserved random variables, and calculates the conditional distribution of unknowns given knowns and model specifications using Bayes’ theorem* ([Rubin, 1984](https://projecteuclid.org/euclid.aos/1176346785)).

Monty Hall
========================================================
incremental: true
type: lineheight

<br>

<div align="center">
<video width="1200" height="600" controls="controls">
<source src="montyhall.mp4" type="video/mp4">
</video>
<div>

Monty Hall
========================================================
incremental: false
type: lineheight

<div align = "center" style="border:none;">
<img src = "monty1.png" width = 600 height = 600>
</div>

<center> What would you do (intuitively) ? Then, analyse the situation using Bayes theorem. </center>

Monty Hall
========================================================
incremental: true
type: lineheight

This is actually a conditional probability problem... Let's define the following events:

D1: the game hosts opens door n°1 <br>
D2: the game hosts opens door n°2 <br>
D3: the game hosts opens door n°3 <br>

C1: the car is behing door n°1 <br>
C2: the car is behing door n°2 <br>
C3: the car is behing door n°3 <br>

If we choose door n°1 and the host chose door n°3 (*and given that he knows where is the car*), it follows that:

$p(D3|C1)=\dfrac{1}{2}$, $p(D3|C2)=1$, $p(D3|C3)=0$

Monty Hall
========================================================
incremental: true
type: lineheight

We know that $p(V3|P3)=0$, we want to know $p(V1|P3)$ and $p(V2|P3)$ in order to choose...

$p(V1|P3)=\dfrac{p(P3|V1) \times p(V1)}{p(P3)}=\dfrac{\dfrac{1}{2} \times \dfrac{1}{3}}{\dfrac{1}{2}}=\dfrac{1}{3}$

$p(V2|P3)=\dfrac{p(P3|V2) \times p(V2)}{p(P3)}=\dfrac{1 \times \dfrac{1}{3}}{\dfrac{1}{2}}=\dfrac{2}{3}$

Monty Hall
========================================================
incremental: true
type: lineheight

<div align = "center" style="border:none;">
<img src = "monty2.png" width = 600 height = 600>
</div>

Summary
========================================================
incremental: true
type: lineheight

Our probabilistic intuitions are usually very bad. Instead of relying on them to make decisions, it is safer to rely on logic (e.g., *modus ponens* or *moduls tollens*) and probabilistic rules (the sum rule, the product rule, Bayes' rule). These rules will conduct us to the most logic inference.

In other words: "Don't be clever" (McElreath, 2015).

```{r echo = FALSE, fig.align = "center", out.width = "600px"}
knitr::include_graphics("morale.gif")
```

<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<script>

for(i=0;i<$("section").length;i++) {
if(i==0) continue
$("section").eq(i).append("<p style='font-size:xx-large;position:fixed;right:200px;bottom:50px;'>" + i + "</p>")
}

</script>
